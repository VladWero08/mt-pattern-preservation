{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification**\n",
        "\n",
        "In this notebook, we fine-tune a [XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta#xlm-roberta) model for news genre classification: opinion, report, or satire. Afterwards, the model is evaluated using macro-precision, macro-recall, and macro-F1. Also, the confusion matrix will be plotted.\n",
        "\n",
        "XML-RoBERTa is a multilingual masked language model trained over 2.5TB of filtered CommonCrawl data across 100 languages. The tokenizer does not need to know the language, because the tokens' ID includes that information, thus it can be used for classification without an additional setup."
      ],
      "metadata": {
        "id": "CvR3KfeODgtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "777_snSjFtdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/VladWero08/mt-pattern-preserve/refs/heads/main/data/articles_en.csv -O articles_en.csv"
      ],
      "metadata": {
        "id": "A7WF_HSqGMjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyVoOUfWDeNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "i2hSI7DAIpen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping\n",
        "class_names = [\"opinion\", \"reporting\", \"satire\"]\n",
        "id2label = {0: \"opinion\", 1: \"reporting\", 2: \"satire\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "label2onehot = {\n",
        "    \"opinion\": [1, 0, 0],\n",
        "    \"reporting\": [0, 1, 0],\n",
        "    \"satire\": [0, 0, 1],\n",
        "  }\n",
        "\n",
        "# load tokenizer\n",
        "model_id = \"FacebookAI/xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# load model\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=len(id2label),\n",
        "    problem_type=\"multi_label_classification\",\n",
        ")\n",
        "# update config\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n"
      ],
      "metadata": {
        "id": "L4LMieVFD6EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sports article. Manchester United won 3-0 last night against Real Madrid.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits\n",
        "  prediction_id = logits.argmax().item()\n",
        "  prediction_label = model.config.id2label[prediction_id]\n",
        "\n",
        "print(f\"{text} --- prediction --> {prediction_label}\")"
      ],
      "metadata": {
        "id": "bdHutLhhJc6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "i_iKTYSDJd5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles = pd.read_csv(\"articles_en.csv\")"
      ],
      "metadata": {
        "id": "miPYuAgYJWpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = torch.Tensor([\n",
        "    len(articles) / 3 * articles[\"genre\"].value_counts().iloc[0].item(),\n",
        "    len(articles) / 3 * articles[\"genre\"].value_counts().iloc[1].item(),\n",
        "    len(articles) / 3 * articles[\"genre\"].value_counts().iloc[2].item(),\n",
        "])"
      ],
      "metadata": {
        "id": "UF4hEAgKU1p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsGenreDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, articles: list[str], labels: list[int]) -> None:\n",
        "    global tokenizer\n",
        "\n",
        "    self.encodings = tokenizer(\n",
        "        articles,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> dict:\n",
        "    item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "    item[\"labels\"] = self.labels[idx]\n",
        "    return item\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "CGf7cEEmQURk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NewsGenreDataset(\n",
        "    [article for article in articles[\"full_articles\"].values],\n",
        "    [label2onehot[genre] for genre in articles[\"genre\"].values],\n",
        ")"
      ],
      "metadata": {
        "id": "7fItfy_pRMAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into train and validation\n",
        "train_idxs, validation_idxs = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_idxs)\n",
        "validation_dataset = torch.utils.data.Subset(dataset, validation_idxs)"
      ],
      "metadata": {
        "id": "vU80M3koR0Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "Gq7TRU0LJfU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassWeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "      global class_weights\n",
        "\n",
        "      labels = inputs.get(\"labels\")\n",
        "      # forward pass\n",
        "      outputs = model(**inputs)\n",
        "      logits = outputs.get('logits')\n",
        "      # compute custom loss\n",
        "      loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "      loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "      return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "fggrz6NkWaqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=1e-4,\n",
        ")"
      ],
      "metadata": {
        "id": "uZ1S2tEgJpdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ClassWeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "mF3pI2GKScj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=torch.utils.data.Subset(train_dataset, range(5)), # Fixed: Create a Subset to maintain Dataset type\n",
        "    eval_dataset=torch.utils.data.Subset(train_dataset, range(2)),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "o1X4eAMIShxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "1qFasJnzJkej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the prediction logits\n",
        "val_predictions = trainer.predict(validation_dataset)\n",
        "# compute the prediction classes\n",
        "val_predictions_labels = torch.argmax(torch.tensor(val_predictions.predictions), dim=1)\n",
        "# extract the true labels from the validation dataset\n",
        "val_true_labels = torch.argmax(dataset[validation_dataset.indices][\"labels\"], dim=1)"
      ],
      "metadata": {
        "id": "26rXxmBwJqqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute metrics\n",
        "precision = precision_score(val_true_labels, val_predictions_labels, average=\"macro\")\n",
        "recall = recall_score(val_true_labels, val_predictions_labels, average=\"macro\")\n",
        "f1 = f1_score(val_true_labels, val_predictions_labels, average=\"macro\")\n",
        "cm = confusion_matrix(val_true_labels, val_predictions_labels)\n"
      ],
      "metadata": {
        "id": "bNgMSNroZuqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Metrics - average = macro\")\n",
        "print(\"-------------------------\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "6qu8YH17dSQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vdTESue-deuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}