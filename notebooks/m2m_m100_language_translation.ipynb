{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Language translation**\n",
        "\n",
        "In this notebook, we use the [M2M-100](https://huggingface.co/facebook/m2m100_418M) multilingual translation model to translate the news article from English into: French, German, Spanish, Polish, Russian.\n",
        "\n",
        "This represents the first step for our first experiment. With this translated articles, we should afterwards train a classifier to group them into an opinion, report or satire piece."
      ],
      "metadata": {
        "id": "ra-qucOULdDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "QVMk5s2NOtXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "HPol_odfNQcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QnpMowhJOSNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate Chinesse to English\n",
        "text = \"生活就像一盒巧克力。\"\n",
        "\n",
        "# tokenize the input using the Chinesse version\n",
        "tokenizer.src_lang = \"zh\"\n",
        "encoded_zh = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "# encode the tokens from the input text\n",
        "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
        "# decode the tokens into the translation\n",
        "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(f\"ZH: {text} ---> EN: {translation[0]}\")"
      ],
      "metadata": {
        "id": "gWEy6_mePPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**\n"
      ],
      "metadata": {
        "id": "XdAxucdlQPT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/VladWero08/mt-pattern-preserve/refs/heads/main/data/articles_en.csv -O articles_en.csv"
      ],
      "metadata": {
        "id": "zkV-bRyaQOm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_en = pd.read_csv(\"articles_en.csv\")"
      ],
      "metadata": {
        "id": "d5Oc_0NfWWJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = articles_en.iloc[0][\"full_articles\"]\n",
        "sentences = sent_tokenize(text)\n",
        "print(f\"Split into {len(sentences)} sentences\")"
      ],
      "metadata": {
        "id": "Dgm-d-YVCBL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "tokenizer.src_lang = \"en\"\n",
        "\n",
        "for sent in sentences:\n",
        "    if len(sent.strip()) < 3:\n",
        "        continue\n",
        "    encoded = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    generated = model.generate(\n",
        "      **encoded,\n",
        "      forced_bos_token_id=tokenizer.get_lang_id(\"fr\"),\n",
        "      max_new_tokens=128,\n",
        "      num_beams=4\n",
        "    )\n",
        "    translated = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
        "    translations.append(translated)\n",
        "\n",
        "full_translation = \" \".join(translations)\n",
        "print(f\"FR: {full_translation}\")\n"
      ],
      "metadata": {
        "id": "dIWNhtNECDYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Translation**"
      ],
      "metadata": {
        "id": "NPVf9nuDms6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all the translation will be done between EN -> target language\n",
        "# target_languages = [\"fr\", \"de\", \"es\", \"pl\", \"ru\"]\n",
        "target_languages = [\"pl\", \"ru\"]\n",
        "tokenizer.src_lang = \"en\"\n",
        "batch_size = 50\n",
        "\n",
        "for target_language in target_languages:\n",
        "    target_language_dataset = []\n",
        "\n",
        "    for start_idx in range(0, len(articles_en), batch_size):\n",
        "        end_idx = min(start_idx + batch_size, len(articles_en))\n",
        "\n",
        "        # extract id, genre and articles batches\n",
        "        batch_ids = articles_en[\"id\"].iloc[start_idx:end_idx]\n",
        "        batch_genres = articles_en[\"genre\"].iloc[start_idx:end_idx]\n",
        "        batch_articles = articles_en[\"full_articles\"].iloc[start_idx:end_idx]\n",
        "\n",
        "        print(f\"Translating batch {start_idx + 1} to {end_idx}...\")\n",
        "\n",
        "        for id_, genre, article in zip(batch_ids, batch_genres, batch_articles):\n",
        "            # break the article into sentences before feeding it to the translation model,\n",
        "            # because articles can get larger than the tokenizer.model_max_length\n",
        "            sentences = sent_tokenize(article)\n",
        "            translation = []\n",
        "\n",
        "            # translate each chunk that concatenate all translations\n",
        "            for sentence in sentences:\n",
        "              encoded = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "              generated_tokens = model.generate(\n",
        "                  **encoded,\n",
        "                  forced_bos_token_id=tokenizer.get_lang_id(target_language),\n",
        "                  max_new_tokens=128,\n",
        "                  num_beams=5,\n",
        "              )\n",
        "              translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "              translation.append(translated)\n",
        "\n",
        "            translation = \" \".join(translation)\n",
        "            target_language_dataset.append({\n",
        "                \"id\": id_,\n",
        "                \"genre\": genre,\n",
        "                \"full_articles\": translation\n",
        "            })\n",
        "\n",
        "    # Save the translated dataset\n",
        "    output_csv = f\"articles_{target_language}.csv\"\n",
        "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=[\"id\", \"genre\", \"full_articles\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(target_language_dataset)\n",
        "    print(f\"Successfully saved .csv for EN -> {target_language.upper()} translations!\")\n"
      ],
      "metadata": {
        "id": "z5PHIBzCQ3hW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}